{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: True\n",
      "GPU usada: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Versi√≥n CUDA de PyTorch: 12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU usada:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Versi√≥n CUDA de PyTorch:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4bbbcb5f654e42288e4f7b54698558e5",
      "2575551347934daa9f104302c2b59871",
      "68b2db3f692c4c0d87d2de914eac1a1f",
      "6ceacda6b1aa4bb5959a21d1672c8a18",
      "c1da56960c194f10b556be5de4c30305",
      "8a846d63dea94b62bbfab1a9372316f0",
      "463a5e287e8f490a8cc9977be2393b6a",
      "a4907817f15a480c8b6c3efff2d2aa90",
      "b6b541fbf7534208b3484fe0ab4df59e",
      "c30b471379c944cd8bd43cc879ed432e",
      "d2de5481f86942f98b9a7f66cb0aab64"
     ]
    },
    "executionInfo": {
     "elapsed": 2103952,
     "status": "ok",
     "timestamp": 1761251152729,
     "user": {
      "displayName": "jesus gutierrez",
      "userId": "14309361231762044459"
     },
     "user_tz": -120
    },
    "id": "_bPeOGwih3vQ",
    "outputId": "9fdba08e-06f2-42c2-e40a-c2923a179701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'You are a Linux terminal emulator.\\nThe current user is \"Ubuntu_user\" and the hostname is \"Ubuntu\". \\nFollow these rules:\\n1. Output only what a real Linux shell would print to stdout/stderr for the given input.\\n2. Respond to all valid Linux commands exactly as a real terminal would.\\n3. For invalid or non-Linux commands, return the typical shell error.\\n4. Assume a standard Linux environment with typical file structures, system utilities, and commands.\\n5. For commands that require interaction (vim, top, nano, passwd), return the short non-interactive error message a real shell would show.\\n6. Every output must end with a line containing only the current working directory path.', 'role': 'system'}, {'content': 'ls -la', 'role': 'user'}, {'content': 'total 54\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 29 12:23 .\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Sep 15 14:23 ..\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 03 08:47 Documents\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Sep 08 21:54 Downloads\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 27 13:52 Projects\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Sep 14 17:33 scripts\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 11562 Oct 01 02:53 README.md\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 12943 Sep 13 12:28 .bashrc\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 19598 Sep 09 05:24 .profile\\nubuntu_user@Ubuntu:~$', 'role': 'assistant'}, {'content': 'cat README.md', 'role': 'user'}, {'content': '# Project README\\n\\nRun \"./install.sh\" to setup the environment.\\nubuntu_user@Ubuntu:~$', 'role': 'assistant'}, {'content': 'ls -la', 'role': 'user'}, {'content': 'total 58\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 02 21:24 .\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Sep 13 09:46 ..\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 22 02:35 Documents\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Sep 10 01:45 Downloads\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 12 02:36 Projects\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 20 08:51 scripts\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 11956 Sep 02 18:28 README.md\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 11981 Sep 30 23:42 install.sh\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 6006 Sep 21 19:06 .bashrc\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 11297 Oct 18 05:05 .profile\\nubuntu_user@Ubuntu:~$', 'role': 'assistant'}, {'content': 'cat install.sh', 'role': 'user'}, {'content': '#!/bin/bash\\n./configure\\nmake\\nmake install\\n\\nubuntu_user@Ubuntu:~$', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# üì¶ Carga y formateo del dataset tipo \"shell emulator\"\n",
    "# ================================================\n",
    "import json, re\n",
    "from datasets import Dataset\n",
    "\n",
    "# Ruta a tu dataset (aj√∫stala si es necesario)\n",
    "dataset_path = \"shell_dataset.jsonl\"\n",
    "\n",
    "# Cargar cada l√≠nea del JSONL\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [json.loads(line) for line in f]\n",
    "\n",
    "# ================================================\n",
    "# üîç Funci√≥n para parsear las conversaciones\n",
    "# Convierte texto plano con etiquetas System/Human/AI ‚Üí formato messages est√°ndar\n",
    "# ================================================\n",
    "def parse_conversation(text: str):\n",
    "    messages = []\n",
    "    current_role = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in text.strip().splitlines():\n",
    "        match = re.match(r\"^(System|Human|AI):\\s*(.*)\", line)\n",
    "        if match:\n",
    "            # Si cambiamos de rol, guardamos el bloque anterior\n",
    "            if current_role and buffer:\n",
    "                messages.append({\"role\": current_role, \"content\": \"\\n\".join(buffer).strip()})\n",
    "                buffer = []\n",
    "            role = match.group(1)\n",
    "            # Mapeamos a roles est√°ndar\n",
    "            current_role = (\n",
    "                \"system\" if role == \"System\"\n",
    "                else \"user\" if role == \"Human\"\n",
    "                else \"assistant\"\n",
    "            )\n",
    "            buffer.append(match.group(2))\n",
    "        else:\n",
    "            # L√≠nea sin etiqueta: pertenece al mensaje actual\n",
    "            buffer.append(line)\n",
    "    \n",
    "    # Guardar el √∫ltimo bloque\n",
    "    if current_role and buffer:\n",
    "        messages.append({\"role\": current_role, \"content\": \"\\n\".join(buffer).strip()})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# üß† Convertir todas las conversaciones\n",
    "# ================================================\n",
    "parsed_data = []\n",
    "for item in lines:\n",
    "    try:\n",
    "        messages = parse_conversation(item[\"text\"])\n",
    "        if messages:  # solo a√±adimos si hay contenido\n",
    "            parsed_data.append({\"messages\": messages})\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing line: {e}\")\n",
    "        continue\n",
    "\n",
    "# Crear dataset HuggingFace\n",
    "dataset = Dataset.from_list(parsed_data)\n",
    "\n",
    "print(dataset[1])  # ejemplo para verificar\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.3: Fast Mistral patching. Transformers: 4.56.2. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33+f204359.d20251014. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|system|>\\nYou are a Linux terminal emulator.\\nThe current user is \"Ubuntu_user\" and the hostname is \"Ubuntu\". \\nFollow these rules:\\n1. Output only what a real Linux shell would print to stdout/stderr for the given input.\\n2. Respond to all valid Linux commands exactly as a real terminal would.\\n3. For invalid or non-Linux commands, return the typical shell error.\\n4. Assume a standard Linux environment with typical file structures, system utilities, and commands.\\n5. For commands that require interaction (vim, top, nano, passwd), return the short non-interactive error message a real shell would show.\\n6. Every output must end with a line containing only the current working directory path.<|end|>\\n<|user|>\\nls -la<|end|>\\n<|assistant|>\\ntotal 54\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 29 12:23 .\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Sep 15 14:23 ..\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 03 08:47 Documents\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Sep 08 21:54 Downloads\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 27 13:52 Projects\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Sep 14 17:33 scripts\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 11562 Oct 01 02:53 README.md\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 12943 Sep 13 12:28 .bashrc\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 19598 Sep 09 05:24 .profile\\nubuntu_user@Ubuntu:~$<|end|>\\n<|user|>\\ncat README.md<|end|>\\n<|assistant|>\\n# Project README\\n\\nRun \"./install.sh\" to setup the environment.\\nubuntu_user@Ubuntu:~$<|end|>\\n<|user|>\\nls -la<|end|>\\n<|assistant|>\\ntotal 58\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 02 21:24 .\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Sep 13 09:46 ..\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 22 02:35 Documents\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Sep 10 01:45 Downloads\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 12 02:36 Projects\\ndrwxr-xr-x 2 ubuntu_user ubuntu_user 4096 Oct 20 08:51 scripts\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 11956 Sep 02 18:28 README.md\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 11981 Sep 30 23:42 install.sh\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 6006 Sep 21 19:06 .bashrc\\n-rw-r--r-- 1 ubuntu_user ubuntu_user 11297 Oct 18 05:05 .profile\\nubuntu_user@Ubuntu:~$<|end|>\\n<|user|>\\ncat install.sh<|end|>\\n<|assistant|>\\n#!/bin/bash\\n./configure\\nmake\\nmake install\\n\\nubuntu_user@Ubuntu:~$<|end|>\\n']\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# ================================================\n",
    "# üöÄ Cargar modelo base\n",
    "# ================================================\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"microsoft/phi-3-mini-4k-instruct\",\n",
    "    device_map = \"auto\",\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    load_in_8bit = False,\n",
    "    gpu_memory_utilization = 0.85,\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# üß© A√±adir LoRA adapters\n",
    "# ================================================\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\"\n",
    ")\n",
    "# ================================================\n",
    "# üß† Funci√≥n de formateo para el entrenamiento tipo chat\n",
    "# ================================================\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "\n",
    "    # Si \"examples\" es una sola fila del dataset:\n",
    "    if isinstance(examples[\"messages\"][0], dict):\n",
    "        conversations = [examples[\"messages\"]]  # una lista que contiene una sola conversaci√≥n\n",
    "    else:\n",
    "        conversations = examples[\"messages\"]\n",
    "\n",
    "    for conversation in conversations:\n",
    "        formatted = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(formatted)\n",
    "\n",
    "    return texts\n",
    "print(formatting_prompts_func(dataset[1]))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# ‚öôÔ∏è Argumentos de entrenamiento optimizados para Unsloth\n",
    "# ================================================\n",
    "max_seq_length = 2048\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=25,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=\"none\",  # Disable Weights & Biases logging\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5087f3ef93b34f1c851a3edd7f1d78b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=20):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 3 | Total steps = 375\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 29,884,416 of 3,850,963,968 (0.78% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 22:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.105100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.107300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================================\n",
    "# üèãÔ∏è Entrenador SFT (fine-tuning estilo chat)\n",
    "# ================================================\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2985,
     "status": "ok",
     "timestamp": 1761251850214,
     "user": {
      "displayName": "jesus gutierrez",
      "userId": "14309361231762044459"
     },
     "user_tz": -120
    },
    "id": "7vBGnx7DXuN9",
    "outputId": "436c3f3a-85c1-4dca-a620-f01f8822654a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a Linux terminal emulator. \"\"\"The current user is \"unmatru\" and the hostname is \"Ubuntu\"\"\" ...<|end|><|user|> cat install.sh<|end|><|assistant|> #!/bin/bash\n",
      "./configure\n",
      "make\n",
      "make install\n",
      "\n",
      "unmatru@Ubuntu:~$<|end|>\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# üîß Habilitar modo de inferencia r√°pida\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": 'You are a Linux terminal emulator. \"\"\"The current user is \"unmatru\" and the hostname is \"Ubuntu\"\"\" ...'},\n",
    "    {\"role\": \"user\", \"content\": \"cat install.sh\"},\n",
    "]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "error",
     "timestamp": 1761253759844,
     "user": {
      "displayName": "jesus gutierrez",
      "userId": "14309361231762044459"
     },
     "user_tz": -120
    },
    "id": "twEmkIrLZLtD",
    "outputId": "a785800b-c66f-4730-ccad-a1d4a4d3f25f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /workspace/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 371.01it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [05:13<00:00, 156.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/work/gguf_model`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF bf16 to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: llama.cpp found in the system. Skipping installation.\n",
      "Unsloth: Preparing converter script...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.llama_cpp: Unsloth: Identifying llama.cpp gguf supported architectures...\n",
      "INFO:unsloth_zoo.llama_cpp: Unsloth: Applying patches...\n",
      "INFO:unsloth_zoo.llama_cpp: Unsloth: Saving patched script to llama.cpp/unsloth_convert_hf_to_gguf.py\n",
      "INFO:unsloth_zoo.llama_cpp: Unsloth: Parsing arguments from patched script...\n",
      "INFO:unsloth_zoo.llama_cpp: Unsloth: Successfully processed convert_hf_to_gguf.py.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: [1] Converting model into bf16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['phi-3-mini-4k-instruct.BF16.gguf']\n",
      "Unsloth: [2] Converting GGUF bf16 into q4_k_m. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['phi-3-mini-4k-instruct.Q4_K_M.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model phi-3-mini-4k-instruct.Q4_K_M.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'save_directory': 'gguf_model',\n",
       " 'gguf_files': ['phi-3-mini-4k-instruct.Q4_K_M.gguf'],\n",
       " 'modelfile_location': '/workspace/work/Modelfile',\n",
       " 'want_full_precision': False,\n",
       " 'is_vlm': False,\n",
       " 'fix_bos_token': False}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x750978954cd0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 615df376-c958-4933-b01d-7c9861073498)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x750978955fd0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 62c73869-a6c3-4b63-9a73-f1e08e712ee5)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x750978957650>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: a375564d-f6bc-4a69-b8ab-360396a7612f)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x75097895ce10>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 9d22eb13-0cbc-4588-a7e4-ab5387d5c725)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x75097895e590>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: ae319ba4-c45c-44b4-a271-f952f962064d)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x75097895f9d0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 5a8ee8d6-5212-4973-aca6-a56fed904dd0)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6858e2b1740c4141af9ccd1ccbc9a6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x750978890a50>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: c9bec49a-e585-4391-9732-e710c9e3ca26)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509788a6a50>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 3ed53410-5088-4c1e-a025-a5b97a5582eb)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509786a95d0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 4c91b5b7-7a9b-41c4-bff4-edc3fbe7e6ac)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509786aadd0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 52cf3fd0-4ca8-4ff9-9e22-9252eb666600)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509786abe50>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: b0d3d012-fcb2-4ac0-8a33-5652e7f2d1ec)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509786b4c90>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 9b963db1-6faf-494b-9c18-e8dec1a53b10)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/generation_config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509786b6a90>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 726c18e1-b2ad-43ea-bd99-7cb39b6ab710)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509786b7750>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 30c565d3-1d01-40ff-9c95-d2576a590a1f)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509786c4710>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: bfabfff3-6033-40b9-a082-107a0d7ee40f)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x750978969d10>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: d10d9f67-68dd-43ef-8c40-1d93d3e87445)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509786abd90>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: de0fc425-16dd-4857-8e73-df1c0b6dbec4)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509786aae10>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 440b70ec-760f-4eee-9390-19e8baaf0bc3)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/custom_generate/generate.py\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509701fb590>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 2a318db4-bed4-4142-a80d-22bf5e862685)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x750970385a10>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: c9238c65-13f2-4408-b07f-96ef4ea7ec93)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509701e8d90>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: b65d0c69-433b-4a8a-8160-9d5ab89972ab)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509701e2610>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 5735cb89-7a2b-4a6c-b5ad-80d654f8e8da)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509701e17d0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 71fc8db0-e72b-4c1d-9758-b2a786c6dfb0)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7509710c7e90>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 7f6033a9-8dc7-4af6-8735-c4a816b6f71e)')' thrown while requesting HEAD https://huggingface.co/microsoft/phi-3-mini-4k-instruct/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('phi3_shell_full/tokenizer_config.json',\n",
       " 'phi3_shell_full/special_tokens_map.json',\n",
       " 'phi3_shell_full/chat_template.jinja',\n",
       " 'phi3_shell_full/tokenizer.model',\n",
       " 'phi3_shell_full/added_tokens.json',\n",
       " 'phi3_shell_full/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Carga el modelo base original\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\", dtype=\"auto\")\n",
    "\n",
    "# Carga tu adaptador LoRA\n",
    "model = PeftModel.from_pretrained(base_model, \"phi3_shell\")\n",
    "\n",
    "# Fusiona LoRA -> base\n",
    "model = model.merge_and_unload()   # üëà esta l√≠nea es la clave\n",
    "\n",
    "# Guarda el modelo completo fusionado\n",
    "model.save_pretrained(\"phi3_shell_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "tokenizer.save_pretrained(\"phi3_shell_full\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "17XuqtEqBK_9hNvx_Z0v0-Nyh8XdCC8O_",
     "timestamp": 1761246862098
    }
   ]
  },
  "kernelspec": {
   "display_name": "Unsloth GPU",
   "language": "python",
   "name": "unsloth-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2575551347934daa9f104302c2b59871": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a846d63dea94b62bbfab1a9372316f0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_463a5e287e8f490a8cc9977be2393b6a",
      "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=6):‚Äá100%"
     }
    },
    "463a5e287e8f490a8cc9977be2393b6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bbbcb5f654e42288e4f7b54698558e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2575551347934daa9f104302c2b59871",
       "IPY_MODEL_68b2db3f692c4c0d87d2de914eac1a1f",
       "IPY_MODEL_6ceacda6b1aa4bb5959a21d1672c8a18"
      ],
      "layout": "IPY_MODEL_c1da56960c194f10b556be5de4c30305"
     }
    },
    "68b2db3f692c4c0d87d2de914eac1a1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4907817f15a480c8b6c3efff2d2aa90",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b6b541fbf7534208b3484fe0ab4df59e",
      "value": 1000
     }
    },
    "6ceacda6b1aa4bb5959a21d1672c8a18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c30b471379c944cd8bd43cc879ed432e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d2de5481f86942f98b9a7f66cb0aab64",
      "value": "‚Äá1000/1000‚Äá[00:01&lt;00:00,‚Äá650.25‚Äáexamples/s]"
     }
    },
    "8a846d63dea94b62bbfab1a9372316f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4907817f15a480c8b6c3efff2d2aa90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6b541fbf7534208b3484fe0ab4df59e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c1da56960c194f10b556be5de4c30305": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c30b471379c944cd8bd43cc879ed432e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2de5481f86942f98b9a7f66cb0aab64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
